import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import urllib.parse
import re
from datetime import datetime
import os
import json
import base64

# í˜ì´ì§€ ì„¤ì • ë° ìŠ¤íƒ€ì¼
st.set_page_config(
    page_title="ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬",
    page_icon="ğŸ“°",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS ìŠ¤íƒ€ì¼ ì¶”ê°€
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1E88E5;
        margin-bottom: 1rem;
    }
    .sub-header {
        font-size: 1.5rem;
        color: #0D47A1;
        margin-bottom: 1rem;
    }
    .stProgress > div > div {
        background-color: #1E88E5;
    }
    .css-1adrfps {
        background-color: #f0f2f6;
        border-radius: 10px;
        padding: 20px;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
    }
    .result-container {
        background-color: #ffffff;
        border-radius: 5px;
        padding: 15px;
        margin-bottom: 10px;
        border-left: 4px solid #1E88E5;
    }
    .css-1v3fvcr {
        background-color: #f9f9f9;
    }
    .stButton>button {
        background-color: #1E88E5;
        color: white;
        border-radius: 5px;
        border: none;
        padding: 0.5rem 1rem;
        font-weight: bold;
    }
    .stButton>button:hover {
        background-color: #0D47A1;
    }
</style>
""", unsafe_allow_html=True)

# ì•± ì œëª©
st.markdown('<div class="main-header">ğŸ“° ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬</div>', unsafe_allow_html=True)
st.markdown('ë„¤ì´ë²„ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰ì–´ì™€ ì›í•˜ëŠ” ê°œìˆ˜ë§Œí¼ ìˆ˜ì§‘í•˜ì—¬ CSV ë˜ëŠ” Excel íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.')

# ê²€ìƒ‰ ê¸°ë¡ ë¡œë“œ í•¨ìˆ˜
@st.cache_data(show_spinner=False)
def load_search_history():
    config_file = os.path.join(os.path.expanduser("~"), "naver_news_crawler_streamlit.json")
    if os.path.exists(config_file):
        try:
            with open(config_file, "r", encoding="utf-8") as f:
                config = json.load(f)
                return config.get("search_history", [])
        except Exception as e:
            st.error(f"ê²€ìƒ‰ ê¸°ë¡ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return []
    return []

# ê²€ìƒ‰ ê¸°ë¡ ì €ì¥ í•¨ìˆ˜
def save_search_history(history):
    config_file = os.path.join(os.path.expanduser("~"), "naver_news_crawler_streamlit.json")
    try:
        with open(config_file, "w", encoding="utf-8") as f:
            json.dump({"search_history": history}, f, ensure_ascii=False, indent=2)
    except Exception as e:
        st.error(f"ê²€ìƒ‰ ê¸°ë¡ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")

# í¬ë¡¤ë§ í•¨ìˆ˜
def crawl_naver_news(query, count, progress_bar, status_text):
    try:
        # ê²°ê³¼ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
        news_list = []
        
        # ì¸ì½”ë”©ëœ ê²€ìƒ‰ì–´
        encoded_query = urllib.parse.quote(query)
        
        # í•„ìš”í•œ í˜ì´ì§€ ìˆ˜ ê³„ì‚° (í•œ í˜ì´ì§€ì— 10ê°œ ê¸°ì‚¬)
        pages_needed = (count + 9) // 10
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7'
        }
        
        collected = 0
        
        for page in range(1, pages_needed + 1):
            if collected >= count:
                break
            
            # í˜ì´ì§€ë‹¹ 10ê°œì”©, ì‹œì‘ ì¸ë±ìŠ¤ ê³„ì‚°
            start = (page - 1) * 10 + 1
            
            # ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ URL
            url = f"https://search.naver.com/search.naver?where=news&sm=tab_jum&query={encoded_query}&start={start}"
            
            try:
                response = requests.get(url, headers=headers)
                response.raise_for_status()  # ì˜¤ë¥˜ ê²€ì‚¬
                soup = BeautifulSoup(response.text, 'html.parser')
                
                status_text.text(f"í˜ì´ì§€ {page} ë¶„ì„ ì¤‘... (ìƒíƒœ ì½”ë“œ: {response.status_code})")
                
                # ë‰´ìŠ¤ ê¸°ì‚¬ ì»¨í…Œì´ë„ˆ ì°¾ê¸° (ìµœì‹  ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ êµ¬ì¡°)
                news_items = soup.select("div.news_area")
                
                if not news_items:
                    status_text.text(f"í˜ì´ì§€ {page}ì—ì„œ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. êµ¬ì¡° í™•ì¸ í•„ìš”.")
                    continue
                    
                status_text.text(f"í˜ì´ì§€ {page}ì—ì„œ {len(news_items)}ê°œ ë‰´ìŠ¤ í•­ëª© ë°œê²¬")
                
                for item in news_items:
                    if collected >= count:
                        break
                    
                    # ì œëª© ì¶”ì¶œ
                    title_elem = item.select_one("a.news_tit")
                    if not title_elem:
                        continue
                        
                    title = title_elem.get_text(strip=True)
                    news_url = title_elem.get('href', '')
                    
                    # ì–¸ë¡ ì‚¬ ì¶”ì¶œ
                    press_elem = item.select_one("a.press")
                    press_name = press_elem.get_text(strip=True) if press_elem else "ì •ë³´ ì—†ìŒ"
                    
                    # ë‚ ì§œ ì •ë³´ ì¶”ì¶œ
                    date_time = "ë‚ ì§œ ì •ë³´ ì—†ìŒ"
                    info_group = item.select_one("div.news_info")
                    if info_group:
                        date_spans = info_group.select("span.info")
                        for span in date_spans:
                            date_text = span.get_text(strip=True)
                            
                            # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ ì²˜ë¦¬
                            if "ë¶„ ì „" in date_text or "ì‹œê°„ ì „" in date_text:
                                date_time = datetime.now().strftime("%Y-%m-%d")
                                break
                            elif "ì¼ ì „" in date_text:
                                date_time = datetime.now().strftime("%Y-%m-%d")
                                break
                            else:
                                # YYYY.MM.DD. í˜•ì‹ ì²˜ë¦¬
                                date_match = re.search(r'(\d{4})\.(\d{2})\.(\d{2})\.?', date_text)
                                if date_match:
                                    year, month, day = date_match.groups()
                                    date_time = f"{year}-{month}-{day}"
                                    break
                    
                    # ë³¸ë¬¸ ë¯¸ë¦¬ë³´ê¸° ì¶”ì¶œ (ìˆëŠ” ê²½ìš°)
                    desc_elem = item.select_one("div.news_dsc")
                    description = desc_elem.get_text(strip=True) if desc_elem else "ë³¸ë¬¸ ë¯¸ë¦¬ë³´ê¸° ì—†ìŒ"
                    
                    # ë„¤ì´ë²„ ë‰´ìŠ¤ ë§í¬ ì°¾ê¸° (ìˆëŠ” ê²½ìš°)
                    naver_news_link = news_url  # ê¸°ë³¸ ë§í¬
                    info_group = item.select_one("div.info_group")
                    if info_group:
                        for link in info_group.find_all('a'):
                            href = link.get('href', '')
                            if 'news.naver.com' in href:
                                naver_news_link = href
                                break
                    
                    # ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                    news_list.append({
                        "ì œëª©": title,
                        "ì–¸ë¡ ì‚¬": press_name,
                        "ë‚ ì§œ": date_time,
                        "ë¯¸ë¦¬ë³´ê¸°": description,
                        "ë§í¬": naver_news_link
                    })
                    
                    collected += 1
                    
                    # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
                    progress_bar.progress(collected / count if count > 0 else 1.0)
                    status_text.text(f"í¬ë¡¤ë§ ì¤‘... ({collected}/{count})")
                
            except requests.exceptions.RequestException as e:
                status_text.text(f"í˜ì´ì§€ {page} ìš”ì²­ ì˜¤ë¥˜: {str(e)}")
                time.sleep(2)  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë” ê¸´ ëŒ€ê¸° ì‹œê°„
                continue
            
            # í˜ì´ì§€ ê°„ ê°„ê²© ë‘ê¸° (ë´‡ ê°ì§€ ë°©ì§€)
            time.sleep(1.5)
            
        return news_list
    
    except Exception as e:
        st.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        return []

# íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë§í¬ ìƒì„± í•¨ìˆ˜
def get_table_download_link(df, filename, file_format):
    if file_format == 'CSV':
        csv = df.to_csv(index=False)
        b64 = base64.b64encode(csv.encode('utf-8-sig')).decode()
        href = f'<a href="data:file/csv;base64,{b64}" download="{filename}.csv" class="download-link">ë‹¤ìš´ë¡œë“œ {filename}.csv</a>'
    else:  # Excel
        output = BytesIO()
        writer = pd.ExcelWriter(output, engine='xlsxwriter')
        df.to_excel(writer, index=False, sheet_name='Sheet1')
        writer.close()
        excel_data = output.getvalue()
        b64 = base64.b64encode(excel_data).decode()
        href = f'<a href="data:application/vnd.openxmlformats-officedocument.spreadsheetml.sheet;base64,{b64}" download="{filename}.xlsx" class="download-link">ë‹¤ìš´ë¡œë“œ {filename}.xlsx</a>'
    return href

# ë©”ì¸ ì•± ë¡œì§
def main():
    # ì‚¬ì´ë“œë°”
    st.sidebar.markdown('<div class="sub-header">ì„¤ì •</div>', unsafe_allow_html=True)
    
    # ê²€ìƒ‰ ê¸°ë¡ ë¡œë“œ
    search_history = load_search_history()
    
    # ê²€ìƒ‰ì–´ ì…ë ¥ (ê²€ìƒ‰ ê¸°ë¡ í¬í•¨)
    if search_history:
        search_options = ["ì§ì ‘ ì…ë ¥"] + search_history
        search_option = st.sidebar.selectbox("ê²€ìƒ‰ ë°©ì‹ ì„ íƒ:", search_options)
        
        if search_option == "ì§ì ‘ ì…ë ¥":
            query = st.sidebar.text_input("ê²€ìƒ‰ì–´ ì…ë ¥:", placeholder="ê²€ìƒ‰í•  í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”")
        else:
            query = search_option
            st.sidebar.text_input("ê²€ìƒ‰ì–´:", value=query, disabled=True)
    else:
        query = st.sidebar.text_input("ê²€ìƒ‰ì–´ ì…ë ¥:", placeholder="ê²€ìƒ‰í•  í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”")
    
    # ë‰´ìŠ¤ ê°œìˆ˜ ì„¤ì •
    count = st.sidebar.slider("í¬ë¡¤ë§í•  ë‰´ìŠ¤ ê°œìˆ˜:", min_value=1, max_value=100, value=20)
    
    # íŒŒì¼ í˜•ì‹ ì„ íƒ
    file_format = st.sidebar.radio("ì €ì¥ íŒŒì¼ í˜•ì‹:", ["Excel", "CSV"])
    
    # íŒŒì¼ ì´ë¦„ ì„¤ì •
    default_filename = f"ë„¤ì´ë²„ë‰´ìŠ¤_{query}_{datetime.now().strftime('%Y%m%d')}" if query else "ë„¤ì´ë²„ë‰´ìŠ¤"
    filename = st.sidebar.text_input("íŒŒì¼ëª…:", value=default_filename)
    
    # ê²€ìƒ‰ ê¸°ë¡ ê´€ë¦¬
    with st.sidebar.expander("ê²€ìƒ‰ ê¸°ë¡ ê´€ë¦¬"):
        if st.button("ëª¨ë“  ê²€ìƒ‰ ê¸°ë¡ ì‚­ì œ"):
            search_history = []
            save_search_history(search_history)
            st.success("ëª¨ë“  ê²€ìƒ‰ ê¸°ë¡ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
            st.experimental_rerun()
        
        if query and st.button("í˜„ì¬ ê²€ìƒ‰ì–´ ì‚­ì œ"):
            if query in search_history:
                search_history.remove(query)
                save_search_history(search_history)
                st.success(f"'{query}' ê²€ìƒ‰ì–´ê°€ ê¸°ë¡ì—ì„œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
                st.experimental_rerun()
    
    # ë©”ì¸ ì»¨í…ì¸ 
    col1, col2 = st.columns([3, 1])
    
    with col2:
        st.markdown('<div class="sub-header">ì‹¤í–‰</div>', unsafe_allow_html=True)
        start_button = st.button("í¬ë¡¤ë§ ì‹œì‘", type="primary", use_container_width=True)
    
    with col1:
        # ì§„í–‰ ìƒíƒœ í‘œì‹œ
        progress_placeholder = st.empty()
        progress_bar = progress_placeholder.progress(0)
        status_placeholder = st.empty()
        status_text = status_placeholder.text("ëŒ€ê¸° ì¤‘...")
    
    # ê²°ê³¼ í‘œì‹œ ì˜ì—­
    result_container = st.container()
    
    # í¬ë¡¤ë§ ì‹œì‘
    if start_button:
        if not query:
            st.error("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        else:
            # ê²€ìƒ‰ ê¸°ë¡ì— ì¶”ê°€
            if query not in search_history:
                search_history.insert(0, query)
                # ìµœëŒ€ 20ê°œ ìœ ì§€
                if len(search_history) > 20:
                    search_history = search_history[:20]
                save_search_history(search_history)
            elif query in search_history:
                # ì´ë¯¸ ì¡´ì¬í•˜ë©´ ë§¨ ì•ìœ¼ë¡œ ì´ë™
                search_history.remove(query)
                search_history.insert(0, query)
                save_search_history(search_history)
            
            # í¬ë¡¤ë§ ì‹œì‘
            with st.spinner("ë‰´ìŠ¤ í¬ë¡¤ë§ ì¤‘..."):
                news_list = crawl_naver_news(query, count, progress_bar, status_text)
            
            # ê²°ê³¼ ì²˜ë¦¬
            if news_list:
                df = pd.DataFrame(news_list)
                
                with result_container:
                    st.markdown('<div class="sub-header">í¬ë¡¤ë§ ê²°ê³¼</div>', unsafe_allow_html=True)
                    
                    # ê²°ê³¼ í†µê³„
                    st.markdown(f"**ì´ {len(news_list)}ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.**")
                    
                    # ë‹¤ìš´ë¡œë“œ ë§í¬
                    from io import BytesIO
                    download_link = get_table_download_link(df, filename, file_format)
                    st.markdown(download_link, unsafe_allow_html=True)
                    
                    # ë°ì´í„° í…Œì´ë¸”ë¡œ í‘œì‹œ
                    st.dataframe(df, use_container_width=True)
                
                # ì§„í–‰ í‘œì‹œ ì—…ë°ì´íŠ¸
                status_text.text(f"í¬ë¡¤ë§ ì™„ë£Œ! {len(news_list)}ê°œ ë‰´ìŠ¤ ì €ì¥ë¨")
                progress_bar.progress(1.0)
            else:
                status_text.text("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ê±°ë‚˜ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
                st.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë¥¼ ì‹œë„í•´ë³´ì„¸ìš”.")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")